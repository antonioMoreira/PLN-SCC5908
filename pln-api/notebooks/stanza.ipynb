{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d992382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import cast, List\n",
    "\n",
    "import copy\n",
    "import rich\n",
    "import torch\n",
    "import stanza\n",
    "import pandas as pd\n",
    "from stanza import Document\n",
    "from rich.console import Console\n",
    "from stanza.models.common.doc import Token, Word\n",
    "\n",
    "# stanza.download('pt') # download Portuguese model\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d06a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4d705",
   "metadata": {},
   "source": [
    "# GramatiQuizz\n",
    "\n",
    "> Esta aplicação busca coletar dados para uma pesquisa de mestrado na Universidade de São Paulo.\n",
    "\n",
    "Bem vindo ao GramatiQuizz. Instruções:\n",
    "\n",
    "1. Selecionar sua série;\n",
    "2. Ler a frase proposta para o teste;\n",
    "3. Para cada palavra da frase você deve selecionar a classe morfossintática que ela pertence;\n",
    "\n",
    "Considerações:\n",
    "- Não se preocupe se você errar;\n",
    "- Você pode responder a ordem que quiser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf4030",
   "metadata": {},
   "source": [
    "# Universal Dependencies TAGs to Portuguese\n",
    "\n",
    "| UPOS | Portuguese | English |\n",
    "|------|------------|---------|\n",
    "| ADJ | Adjetivo | Adjective |\n",
    "| ADP | Preposição | Adposition |\n",
    "| ADV | Advérbio | Adverb |\n",
    "| AUX | Verbo Auxiliar ou de Ligação $^1$ | Auxiliary |\n",
    "| CCONJ | Conjunção Coordenativa | Coordinating Conjunction |\n",
    "| DET | Artigo | Determiner |\n",
    "| INTJ | Interjeição | Interjection |\n",
    "| NOUN | Substantivo | Noun |\n",
    "| NUM | Numeral | Numeral |\n",
    "| PART | Partícula | Particle |\n",
    "| PRON | Pronome | Pronoun |\n",
    "| PROPN | Nome Próprio | Proper Noun |\n",
    "| PUNCT | Pontuação | Punctuation |\n",
    "| SCONJ | Conjunção Subordinativa | Subordinating Conjunction |\n",
    "| SYM | Símbolo | Symbol |\n",
    "| VERB | Verbo | Verb |\n",
    "| X | Outro | Other |\n",
    "\n",
    "\n",
    "$^1$ If `deprel` = `AUX`, it is \"Verbo Auxiliar\" | If `deprel` = `cop` (from `copula`), it is \"Verbo de Ligação\" \n",
    "\n",
    "# Samples\n",
    "- `ADV`     : $\\text{Ele normalmente come café da manhã antes de ir trabalhar.}$\n",
    "- `ADV`     : \"Ela frequentemente se atrasava para a reunião, o que gerava alguma confusão.\" \n",
    "- `AUX`     : \"Luís havia passado a noite em claro.\" (\"Verbo Auxiliar\")\n",
    "- `AUX`     : \"Ele parece estar cansado.\" (\"Verbo de Ligação\")\n",
    "- `CCONJ`   : \"Antonio estudou muito, portanto tirou uma boa nota.\"\n",
    "- `INTJ`    : \"Oi, você pode me ajudar?\"\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c372d",
   "metadata": {},
   "source": [
    "# Stanza 101\n",
    "\n",
    "## Document Methods\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `add_property` | Add a property to the document |\n",
    "| `build_ents` | Build entities in the document |\n",
    "| `coref` | Access coreference information |\n",
    "| `entities` | Get entities from the document |\n",
    "| `ents` | Access named entities |\n",
    "| `from_serialized` | Create document from serialized data |\n",
    "| `get` | Get a property value |\n",
    "| `get_mwt_expansions` | Get multi-word token expansions |\n",
    "| `iter_tokens` | Iterate over tokens in the document |\n",
    "| `iter_words` | Iterate over words in the document |\n",
    "| `lang` | Get document language |\n",
    "| `mark_whitespace` | Mark whitespace in the document |\n",
    "| `num_tokens` | Get total number of tokens |\n",
    "| `num_words` | Get total number of words |\n",
    "| `reindex_sentences` | Reindex sentences in the document |\n",
    "| `sentence_comments` | Access sentence comments |\n",
    "| `sentences` | Access sentences in the document |\n",
    "| `set` | Set a property value |\n",
    "| `set_mwt_expansions` | Set multi-word token expansions |\n",
    "| `sort_features` | Sort morphological features |\n",
    "| `text` | Get raw text of the document |\n",
    "| `to_dict` | Convert document to dictionary |\n",
    "| `to_serialized` | Convert document to serialized format |\n",
    "\n",
    "## Token Methods\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `add_property` | Add a property to the token |\n",
    "| `consolidate_whitespace` | Consolidate whitespace in the token |\n",
    "| `end_char` | Get the ending character position |\n",
    "| `id` | Get the token ID |\n",
    "| `is_mwt` | Check if token is a multi-word token |\n",
    "| `manual_expansion` | Access manual expansion data |\n",
    "| `misc` | Access miscellaneous token information |\n",
    "| `multi_ner` | Access multi-layer named entity recognition |\n",
    "| `ner` | Access named entity recognition tag |\n",
    "| `pretty_print` | Print token in a formatted way |\n",
    "| `sent` | Get the sentence containing this token |\n",
    "| `spaces_after` | Get spaces after the token |\n",
    "| `spaces_before` | Get spaces before the token |\n",
    "| `start_char` | Get the starting character position |\n",
    "| `text` | Get the token text |\n",
    "| `to_conll_text` | Convert token to CoNLL format |\n",
    "| `to_dict` | Convert token to dictionary |\n",
    "| `words` | Access words within the token |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9a8048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:53:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 428kB [00:00, 94.7MB/s]                    \n",
      "2025-05-25 16:53:08 INFO: Downloaded file to /home/antonio/stanza_resources/resources.json\n",
      "2025-05-25 16:53:11 INFO: Loading these models for language: pt (Portuguese):\n",
      "==================================\n",
      "| Processor    | Package         |\n",
      "----------------------------------\n",
      "| tokenize     | bosque          |\n",
      "| mwt          | bosque          |\n",
      "| pos          | bosque_charlm   |\n",
      "| lemma        | bosque_nocharlm |\n",
      "| constituency | cintil_charlm   |\n",
      "| depparse     | bosque_charlm   |\n",
      "==================================\n",
      "\n",
      "2025-05-25 16:53:11 INFO: Using device: cuda\n",
      "2025-05-25 16:53:11 INFO: Loading: tokenize\n",
      "2025-05-25 16:53:13 INFO: Loading: mwt\n",
      "2025-05-25 16:53:13 INFO: Loading: pos\n",
      "2025-05-25 16:53:15 INFO: Loading: lemma\n",
      "2025-05-25 16:53:15 INFO: Loading: constituency\n",
      "2025-05-25 16:53:15 INFO: Loading: depparse\n",
      "2025-05-25 16:53:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('pt') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0710524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 5 | Num words: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Eles\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lemma\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"eles\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"upos\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"PRON\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"feats\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Case=Nom|Gender=Masc|Number=Plur|Person=3|PronType=Prs\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"deprel\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"nsubj\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"start_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"end_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"id\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m\"text\"\u001b[0m: \u001b[32m\"Eles\"\u001b[0m,\n",
       "    \u001b[32m\"lemma\"\u001b[0m: \u001b[32m\"eles\"\u001b[0m,\n",
       "    \u001b[32m\"upos\"\u001b[0m: \u001b[32m\"PRON\"\u001b[0m,\n",
       "    \u001b[32m\"feats\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mCase\u001b[0m\u001b[32m=\u001b[0m\u001b[32mNom\u001b[0m\u001b[32m|\u001b[0m\u001b[32mGender\u001b[0m\u001b[32m=\u001b[0m\u001b[32mMasc\u001b[0m\u001b[32m|\u001b[0m\u001b[32mNumber\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPlur\u001b[0m\u001b[32m|\u001b[0m\u001b[32mPerson\u001b[0m\u001b[32m=\u001b[0m\u001b[32m3\u001b[0m\u001b[32m|\u001b[0m\u001b[32mPronType\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPrs\"\u001b[0m,\n",
       "    \u001b[32m\"head\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m\"deprel\"\u001b[0m: \u001b[32m\"nsubj\"\u001b[0m,\n",
       "    \u001b[32m\"start_char\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m\"end_char\"\u001b[0m: \u001b[1;36m4\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------<class 'stanza.models.common.doc.Token'>-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"têm\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lemma\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ter\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"upos\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"VERB\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"feats\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"deprel\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"root\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"start_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"end_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"id\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m\"text\"\u001b[0m: \u001b[32m\"têm\"\u001b[0m,\n",
       "    \u001b[32m\"lemma\"\u001b[0m: \u001b[32m\"ter\"\u001b[0m,\n",
       "    \u001b[32m\"upos\"\u001b[0m: \u001b[32m\"VERB\"\u001b[0m,\n",
       "    \u001b[32m\"feats\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mMood\u001b[0m\u001b[32m=\u001b[0m\u001b[32mInd\u001b[0m\u001b[32m|\u001b[0m\u001b[32mNumber\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPlur\u001b[0m\u001b[32m|\u001b[0m\u001b[32mPerson\u001b[0m\u001b[32m=\u001b[0m\u001b[32m3\u001b[0m\u001b[32m|\u001b[0m\u001b[32mTense\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPres\u001b[0m\u001b[32m|\u001b[0m\u001b[32mVerbForm\u001b[0m\u001b[32m=\u001b[0m\u001b[32mFin\"\u001b[0m,\n",
       "    \u001b[32m\"head\"\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m\"deprel\"\u001b[0m: \u001b[32m\"root\"\u001b[0m,\n",
       "    \u001b[32m\"start_char\"\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m\"end_char\"\u001b[0m: \u001b[1;36m8\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------<class 'stanza.models.common.doc.Token'>-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"o\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lemma\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"o\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"upos\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"DET\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"feats\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Definite=Def|Gender=Masc|Number=Sing|PronType=Art\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"deprel\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"det\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"start_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"end_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"id\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m\"text\"\u001b[0m: \u001b[32m\"o\"\u001b[0m,\n",
       "    \u001b[32m\"lemma\"\u001b[0m: \u001b[32m\"o\"\u001b[0m,\n",
       "    \u001b[32m\"upos\"\u001b[0m: \u001b[32m\"DET\"\u001b[0m,\n",
       "    \u001b[32m\"feats\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mDefinite\u001b[0m\u001b[32m=\u001b[0m\u001b[32mDef\u001b[0m\u001b[32m|\u001b[0m\u001b[32mGender\u001b[0m\u001b[32m=\u001b[0m\u001b[32mMasc\u001b[0m\u001b[32m|\u001b[0m\u001b[32mNumber\u001b[0m\u001b[32m=\u001b[0m\u001b[32mSing\u001b[0m\u001b[32m|\u001b[0m\u001b[32mPronType\u001b[0m\u001b[32m=\u001b[0m\u001b[32mArt\"\u001b[0m,\n",
       "    \u001b[32m\"head\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "    \u001b[32m\"deprel\"\u001b[0m: \u001b[32m\"det\"\u001b[0m,\n",
       "    \u001b[32m\"start_char\"\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "    \u001b[32m\"end_char\"\u001b[0m: \u001b[1;36m10\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------<class 'stanza.models.common.doc.Token'>-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"poder\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lemma\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"poder\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"upos\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"NOUN\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"feats\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Gender=Masc|Number=Sing\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"deprel\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"obj\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"start_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"end_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"misc\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceAfter=No\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"id\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "    \u001b[32m\"text\"\u001b[0m: \u001b[32m\"poder\"\u001b[0m,\n",
       "    \u001b[32m\"lemma\"\u001b[0m: \u001b[32m\"poder\"\u001b[0m,\n",
       "    \u001b[32m\"upos\"\u001b[0m: \u001b[32m\"NOUN\"\u001b[0m,\n",
       "    \u001b[32m\"feats\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mGender\u001b[0m\u001b[32m=\u001b[0m\u001b[32mMasc\u001b[0m\u001b[32m|\u001b[0m\u001b[32mNumber\u001b[0m\u001b[32m=\u001b[0m\u001b[32mSing\"\u001b[0m,\n",
       "    \u001b[32m\"head\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m\"deprel\"\u001b[0m: \u001b[32m\"obj\"\u001b[0m,\n",
       "    \u001b[32m\"start_char\"\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
       "    \u001b[32m\"end_char\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[32m\"misc\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mSpaceAfter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mNo\"\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------<class 'stanza.models.common.doc.Token'>-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"id\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\".\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"lemma\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\".\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"upos\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"PUNCT\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"head\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"deprel\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"punct\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"start_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"end_char\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"misc\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceAfter=No\"</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"id\"\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "    \u001b[32m\"text\"\u001b[0m: \u001b[32m\".\"\u001b[0m,\n",
       "    \u001b[32m\"lemma\"\u001b[0m: \u001b[32m\".\"\u001b[0m,\n",
       "    \u001b[32m\"upos\"\u001b[0m: \u001b[32m\"PUNCT\"\u001b[0m,\n",
       "    \u001b[32m\"head\"\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[32m\"deprel\"\u001b[0m: \u001b[32m\"punct\"\u001b[0m,\n",
       "    \u001b[32m\"start_char\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[32m\"end_char\"\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "    \u001b[32m\"misc\"\u001b[0m: \u001b[32m\"\u001b[0m\u001b[32mSpaceAfter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mNo\"\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------<class 'stanza.models.common.doc.Token'>-------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phrase = \"Ele normalmente come café da manhã antes de ir trabalhar.\"\n",
    "phrase = \"Ele parece estar cansado.\"\n",
    "phrase = \"Luís havia passado a noite em claro.\"\n",
    "phrase = \"Antonio estudou muito, portanto tirou uma boa nota.\"\n",
    "phrase = \"Os meus livros estão na estante.\"\n",
    "phrase = \"Oi, você pode me ajudar?\"\n",
    "phrase = \"Embora ficasse nervosa, sempre se saía bem.\"\n",
    "phrase = \"Reunião adiada por causa da chuva.\"\n",
    "\n",
    "phrase = str(input(\"Digite uma frase: \"))\n",
    "\n",
    "d:Document = nlp(phrase)\n",
    "\n",
    "print(f\"Num tokens: {d.num_tokens} | Num words: {d.num_words}\")\n",
    "\n",
    "token:Token\n",
    "for token in d.iter_tokens():\n",
    "    words:List[Word] = token.words\n",
    "    console.print(token)\n",
    "    print(f'----------------{type(token)}-------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ab6b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_property',\n",
       " 'consolidate_whitespace',\n",
       " 'end_char',\n",
       " 'id',\n",
       " 'is_mwt',\n",
       " 'manual_expansion',\n",
       " 'misc',\n",
       " 'multi_ner',\n",
       " 'ner',\n",
       " 'pretty_print',\n",
       " 'sent',\n",
       " 'spaces_after',\n",
       " 'spaces_before',\n",
       " 'start_char',\n",
       " 'text',\n",
       " 'to_conll_text',\n",
       " 'to_dict',\n",
       " 'words']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(token) if not m.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8937e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_property',\n",
       " 'build_ents',\n",
       " 'coref',\n",
       " 'entities',\n",
       " 'ents',\n",
       " 'from_serialized',\n",
       " 'get',\n",
       " 'get_mwt_expansions',\n",
       " 'iter_tokens',\n",
       " 'iter_words',\n",
       " 'lang',\n",
       " 'mark_whitespace',\n",
       " 'num_tokens',\n",
       " 'num_words',\n",
       " 'reindex_sentences',\n",
       " 'sentence_comments',\n",
       " 'sentences',\n",
       " 'set',\n",
       " 'set_mwt_expansions',\n",
       " 'sort_features',\n",
       " 'text',\n",
       " 'to_dict',\n",
       " 'to_serialized']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(d) if not m.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1dba59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_translation = {\n",
    "    \"ADJ\"  : \"Adjetivo\",\n",
    "    \"ADP\"  : \"Preposição\",\n",
    "    \"ADV\"  : \"Advérbio\",\n",
    "    \"AUX\"  : {'pass':\"Verbo Passivo\", 'cop':\"Verbo de Ligação\", 'aux:pass':\"Verbo Auxiliar\", 'aux':\"Verbo Auxiliar\"},\n",
    "    \"CCONJ\": \"Conjunção Coordenativa\",\n",
    "    \"DET\"  : \"Artigo\",\n",
    "    \"INTJ\" : \"Interjeição\",\n",
    "    \"NOUN\" : \"Substantivo\",\n",
    "    \"NUM\"  : \"Numeral\",\n",
    "    \"PART\" : \"Partícula\",\n",
    "    \"PRON\" : \"Pronome\",\n",
    "    \"PROPN\": \"Nome Próprio\",\n",
    "    \"PUNCT\": \"Pontuação\",\n",
    "    \"SCONJ\": \"Conjunção Subordinativa\",\n",
    "    \"SYM\"  : \"Símbolo\",\n",
    "    \"VERB\" : \"Verbo\",\n",
    "    \"X\"    : \"Outro\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26808355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Adjetivo', 'Preposição', 'Advérbio', {'pass': 'Verbo Passivo', 'cop': 'Verbo de Ligação', 'aux:pass': 'Verbo Auxiliar', 'aux': 'Verbo Auxiliar'}, 'Conjunção Coordenativa', 'Artigo', 'Interjeição', 'Substantivo', 'Numeral', 'Partícula', 'Pronome', 'Nome Próprio', 'Pontuação', 'Conjunção Subordinativa', 'Símbolo', 'Verbo', 'Outro'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ae9bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translated_tag(word: Word) -> str:\n",
    "    \"\"\"\n",
    "    Retorna a tradução da tag UPOS para português.\n",
    "    \"\"\"\n",
    "    if not str(word.upos) == 'AUX':\n",
    "        return upos_translation[str(word.upos)]\n",
    "    else:\n",
    "        try:\n",
    "            return upos_translation['AUX'][str(word.deprel)]\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}\")\n",
    "            return upos_translation['AUX']['aux']\n",
    "\n",
    "def get_pos_tags(text) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Processa o texto e retorna uma lista de dicionários com a palavra e sua classe gramatical em português.\n",
    "    \"\"\"\n",
    "    doc:Document = nlp(text)\n",
    "    pos_tags = []\n",
    "\n",
    "    token:Token\n",
    "    for token in doc.iter_tokens():\n",
    "        words:List[Word] = token.words\n",
    "        if not token.is_mwt():\n",
    "            word = words[0]\n",
    "            translated_tag = get_translated_tag(word)  # Se não achar, usa a própria tag original\n",
    "            w = {\n",
    "                'word': token.text,\n",
    "                'tag': translated_tag\n",
    "            }\n",
    "        else:\n",
    "            w = {\n",
    "                'word': token.text,\n",
    "                'tag': '+'.join([get_translated_tag(w) for w in words])\n",
    "            }\n",
    "\n",
    "        if w['tag'] == 'Outro':\n",
    "            return []\n",
    "        pos_tags.append(w)\n",
    "\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4758610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase = \"Com certeza você não está dizendo a verdade, não é mesmo?\"\n",
    "# nlp(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e480738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum token encontrado.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "phrase = \"E então, sem que ninguém esperasse, ele simplesmente disse: 'enough!'.\"\n",
    "\n",
    "tags_list = get_pos_tags(phrase)\n",
    "\n",
    "if not tags_list:\n",
    "    print(\"Nenhum token encontrado.\")\n",
    "else:\n",
    "    d = {\n",
    "        'phrase': phrase,\n",
    "        'pos_tags': tags_list\n",
    "    }\n",
    "\n",
    "    # Printing the dictionary as formatted JSON\n",
    "    print(json.dumps(d, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994e8199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'da', 'tag': 'Preposição+Artigo'}\n"
     ]
    }
   ],
   "source": [
    "from sympy import Li\n",
    "\n",
    "phrase = \"Reunião adiada por causa da chuva.\"\n",
    "\n",
    "d:Document = nlp(phrase)\n",
    "\n",
    "t:Token\n",
    "for t in d.iter_tokens():\n",
    "    words:List[Word] = t.words\n",
    "    assert isinstance(words, list)\n",
    "    if t.is_mwt(): # is multi-word token\n",
    "        w = {\n",
    "            'word': t.text,\n",
    "            'tag': '+'.join([get_translated_tag(w) for w in words])\n",
    "        }\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5f8ea",
   "metadata": {},
   "source": [
    "# Read pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/frase_tags.pkl', 'rb') as f:\n",
    "    frase_tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "393a1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"E então, sem que ninguém esperasse, ele simplesmente disse: 'enough!'.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_tags[frase_tags['frase'].str.contains('enough')]['frase'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "815e6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_tags.drop_duplicates(subset=['frase'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "97ba7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase skipped: E então, sem que ninguém esperasse, ele simplesmente disse: 'enough!'.\n",
      "KeyError: 'conj'\n"
     ]
    }
   ],
   "source": [
    "frase_tags = cast(pd.DataFrame, frase_tags)\n",
    "\n",
    "d = []\n",
    "\n",
    "for i,row in frase_tags.iterrows():\n",
    "    word_pos_list = []\n",
    "    for word in row['tags'].iter_words():\n",
    "        translated_pos = upos_translation.get(word.upos, word.upos)\n",
    "        word_pos_list.append({\"word\": word.text, \"tag\": translated_pos})\n",
    "\n",
    "    try:\n",
    "        tags_list = get_pos_tags(row['frase'])\n",
    "        if not tags_list:\n",
    "            print(f\"Phrase skipped: {row['frase']}\")\n",
    "            continue\n",
    "        s = {\n",
    "            'pharase': row['frase'],\n",
    "            'tags' : tags_list,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing phrase: {row['frase']}\")\n",
    "        raise e\n",
    "\n",
    "    d.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4894fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "addon_phrases = [\n",
    "    \"Ele normalmente come café da manhã antes de ir trabalhar.\",\n",
    "    \"Ele parece estar cansado.\",\n",
    "    \"Luís havia passado a noite em claro.\",\n",
    "    \"Antonio estudou muito, portanto tirou uma boa nota.\",\n",
    "    \"Os meus livros estão na estante.\",\n",
    "    \"Oi, você pode me ajudar?\",\n",
    "    \"Embora ficasse nervosa, sempre se saía bem.\",\n",
    "    \"Reunião adiada por causa da chuva.\"\n",
    "]\n",
    "\n",
    "def add_more_phrases(d:List[dict[str, str]], phrases:List[str]) -> List[dict[str, str]]:\n",
    "    d_copy = copy.deepcopy(d)\n",
    "\n",
    "    for phrase in addon_phrases:\n",
    "        try:\n",
    "            tags_list = get_pos_tags(phrase)\n",
    "            if not tags_list:\n",
    "                print(f\"Phrase skipped: {phrase}\")\n",
    "                continue\n",
    "            s = {\n",
    "                'pharase': phrase,\n",
    "                'tags' : tags_list,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing phrase: {phrase}\")\n",
    "            raise e\n",
    "        \n",
    "        d_copy.append(s)\n",
    "\n",
    "    return d_copy\n",
    "    \n",
    "d_add = add_more_phrases(d, addon_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "75ad3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/processed_phrases_2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(d, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "310de3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'O', 'tag': 'Determinante'},\n",
       " {'word': 'gato', 'tag': 'Substantivo'},\n",
       " {'word': 'dorme', 'tag': 'Verbo'},\n",
       " {'word': 'em', 'tag': 'Preposição'},\n",
       " {'word': 'o', 'tag': 'Determinante'},\n",
       " {'word': 'sofá', 'tag': 'Substantivo'},\n",
       " {'word': '.', 'tag': 'Pontuação'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pos_list = []\n",
    "for word in x.iter_words():\n",
    "    translated_pos = upos_translation.get(word.upos, word.upos)\n",
    "    word_pos_list.append({\"word\": word.text, \"tag\": translated_pos})\n",
    "\n",
    "word_pos_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln-api-py31110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
