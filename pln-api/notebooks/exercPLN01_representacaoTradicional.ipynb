{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><center>UNIVERSIDADE DE SÃO PAULO</center></font>\n",
    "<font size=\"6\"><center>INSTITUTO DE CIÊNCIAS MATEMÁTICAS E DE COMPUTAÇÃO</center></font>\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: SCC0633/5908 – Processamento de Linguagem Natural</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Profs. Renato M. Silva e Thiago A. S. Pardo</center></font>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Técnicas tradicionais de representação de texto</center>\n",
    "\n",
    "Neste exercício, usaremos técnicas tradicionais para representar os textos de uma base de dados.\n",
    "\n",
    "### Objetivos de aprendizagem\n",
    "- Entender como fazer o pré-processamento de textos\n",
    "- Implementar as principais técnicas de representação de texto\n",
    "- Entender como calcular similaridade de coseno \n",
    "\n",
    " \n",
    "Primeiro, vamos importar as bibliotecas que serão usadas neste exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # biblioteca usada para realizar tarefas específicas ao SO\n",
    "import re # biblioteca para expressoes regulares\n",
    "\n",
    "import numpy as np # biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # biblioteca usada para trabalhar com dataframes e análise de dados\n",
    "\n",
    "from zipfile import ZipFile # biblioteca para arquivos zipados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos baixar uma base de dados. \n",
    "\n",
    "- Link: <https://github.com/sidleal/porsimplessent>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def download(url, path):\n",
    "\n",
    "    # comando para fazer o download\n",
    "    cmd = ['wget', '-q', url, '-O', '%s' %(path)]\n",
    "\n",
    "    # executa o comando sem exibir a saída na tela\n",
    "    processo = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # aguarda o término do download\n",
    "    stdout, stderr = processo.communicate()\n",
    "\n",
    "    # Verifica se o processo foi concluído com sucesso (código de retorno zero)\n",
    "    if processo.returncode == 0:\n",
    "        print(f'Processo finalizado.')\n",
    "    else:\n",
    "        print(f'Houve um erro: {stderr.decode(\"utf-8\")}')\n",
    "    \n",
    "# especifica o local onde ficarao os arquivos\n",
    "pathFiles = 'dados/'\n",
    "\n",
    "# cria uma pasta onde ficarao os arquivos\n",
    "if not os.path.isdir(pathFiles):\n",
    "    os.mkdir(pathFiles)\n",
    "\n",
    "url = 'https://www.dropbox.com/scl/fi/qnlwhpcqsgoy0m8y23mgz/porsimplessent-master.zip?rlkey=5lb3au2irlz4uv9d6tkbmza0j&st=ko7ho91c&dl=0' \n",
    "pathDataset1 = pathFiles + '/porsimplessent-master.zip'\n",
    "download(url, pathDataset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos descompactar o arquivo que contém a base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(path, pathFolder):\n",
    "\n",
    "    try:\n",
    "        # descompacta ao arquivo\n",
    "        z = ZipFile(path, 'r')\n",
    "        z.extractall(pathFolder)\n",
    "        z.close()\n",
    "\n",
    "        print(\"Arquivo descompactado com sucesso!\")\n",
    "    except:\n",
    "        print(\"Houve um erro ao tentar descompactar o arquivo\")\n",
    "    \n",
    "unzip(pathDataset1, pathFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos importar os dados da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(path):\n",
    "    \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# importa a base de dados de noticias falsas\n",
    "dataset = import_dataset('dados/porsimplessent-master/porsimples/porsimples_sentences.tsv')\n",
    "print('Dados importados com sucesso')\n",
    "\n",
    "print('\\nQtd. de sentenças: %d' %len(dataset))\n",
    "\n",
    "print('\\nPrimeiras sentenças: \\n')\n",
    "for sent in dataset[0:10]:\n",
    "    print('\\n')\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos tratar os textos importados da base de dados. Como o texto está na língua portuguesa, devemos usar uma função de estemização apropriada para a língua portuguesa. Iremos também remover os acentos das palavras. \n",
    "\n",
    "Usaremos alguns módulos da biblioteca [NLTK](https://www.nltk.org/). Para que eles possam ser executados, é necessário fazer o download das bases de dados e pacotes complementares usados pela biblioteca. Para fazer o download de todas os pacotes, use o script `nltk.download('all')`. Para fazer o download apenas do pacote de stopwords e de stemming, use, respecitivamente: `nltk.download('stopwords')` e `nltk.download('rslp')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "import unicodedata # sera usada para remover acentos dos documentos em lingua portuguesa\n",
    "\n",
    "# Download the stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the RSLPStemmer\n",
    "nltk.download('rslp')\n",
    "\n",
    "from nltk.stem import RSLPStemmer # para fazer a estemização em documentos da lingua portuguesa\n",
    "\n",
    "def preprocessing_portuguese(text, stemming = False, stopwords = False):\n",
    "    \"\"\"\n",
    "    Funcao usada para tratar textos escritos na lingua portuguesa\n",
    "    \n",
    "    Parametros: \n",
    "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
    "        \n",
    "        stemming: variavel do tipo boolean que indica se a estemizacao deve ser aplicada ou nao\n",
    "        \n",
    "        stopwords: variavel do tipo boolean que indica se as stopwords devem ser removidas ou nao\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove os acentos das palavras\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    text = u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "    \n",
    "    # remove tags HTML\n",
    "    regex = re.compile('<[^<>]+>')\n",
    "    text = re.sub(regex, \" \", text) \n",
    "    \n",
    "    # normaliza as URLs\n",
    "    regex = re.compile('(http|https)://[^\\s]*')\n",
    "    text = re.sub(regex, \"<URL>\", text)\n",
    "\n",
    "    # normaliza emails\n",
    "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
    "    text = re.sub(regex, \"<EMAIL>\", text)\n",
    "    \n",
    "    # converte todos os caracteres não-alfanuméricos em espaço\n",
    "    regex = re.compile('[^A-Za-z0-9]+') \n",
    "    text = re.sub(regex, \" \", text)\n",
    "    \n",
    "    # normaliza os numeros \n",
    "    regex = re.compile('[0-9]+.[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "    \n",
    "    # normaliza os numeros \n",
    "    regex = re.compile('[0-9]+,[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "    \n",
    "    # normaliza os numeros \n",
    "    regex = re.compile('[0-9]+')\n",
    "    text = re.sub(regex, \"NUMERO\", text)\n",
    "\n",
    "    \n",
    "    # substitui varios espaçamentos seguidos em um só\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # separa o texto em palavras\n",
    "    words = text.split() \n",
    "        \n",
    "    # remove stopwords\n",
    "    if stopwords:\n",
    "\n",
    "        ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "        \n",
    "\n",
    "\n",
    "        ##########################################################################\n",
    "    \n",
    "    # aplica estemização\n",
    "    if stemming: \n",
    "\n",
    "        ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "        \n",
    "\n",
    "\n",
    "        ##########################################################################\n",
    "        \n",
    "    # remove palavras compostas por apenas um caracter\n",
    "    words = text.split() # separa o texto em palavras\n",
    "    words = [ w for w in words if len(w)>1 ]\n",
    "    text = \" \".join( words )\n",
    "    \n",
    "    return text\n",
    "\n",
    "exemplo_noticia = 'Quase 30 anos depois, banhistas assustados estão se afastando do principal balneário de Uruguaiana, na Fronteira Oeste. '\n",
    "print('Antes do preprocessamento: \\n', exemplo_noticia)\n",
    "\n",
    "# executa a função de pré-processsamento para tratar a amostra de texto\n",
    "exemplo_noticia = preprocessing_portuguese(exemplo_noticia, stemming = True, stopwords = True)\n",
    "\n",
    "print('\\nDepois do preprocessamento: \\n', exemplo_noticia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos aplicar a pré-processamento em todos os documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "##########################################################################\n",
    "\n",
    "print(\"\\n\\nPrimeira amostra\")\n",
    "print(dataset2[10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos dar uma análisada na base de dados usando uma nuvem de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "!pip install wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Plote a nuvem de palavras\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos transformar o texto em um vetor de atributos com valores numéricos. Uma das formas de fazer isso é considerar que cada palavra (ou token) da base de dados de treinamento é um atributo que armazena o número de vezes que uma determinada palavra aparece no texto. Na biblioteca `scikit-learn` podemos fazer essa conversão de texto para um vetor de atributos usando a função `skl.feature_extraction.text.CountVectorizer()`. Essa função gera um modelo de vetorização que pode ser ajustado com a base nos dados de treinamento usando a função `fit_transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "print('20 primeiras palavras do vocabulário obtido a partir dos dados de treinamento:\\n')\n",
    "print(vectorizer.get_feature_names_out()[0:20])\n",
    "\n",
    "print('\\nDimensão dos dados vetorizados: ', X.shape)\n",
    "\n",
    "print(X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos converter para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "print(X_tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos converter para binário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin = X.copy()\n",
    "\n",
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "print(X_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busque os documentos mais relevantes para o termo \"praia\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "text = \"carro na praia\"\n",
    "\n",
    "########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Imprime os 10 documentos mais próximos\n",
    "for i in range(10):\n",
    "    print(\"\\n\"+20*\"=\"+\"\\n\")\n",
    "\n",
    "    idxAtual = idxOrd[i]\n",
    "    print(dataset[idxAtual])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
